\documentclass[border=5mm, convert, usenames, dvipsnames,beamer]{standalone}
\usetheme{Madrid}
\usecolortheme{default}
%Information to be included in the title page:
\title{Sample title}
\author{Anonymous}
\institute{Overleaf}
\date{2021}

\usepackage[absolute,overlay]{textpos}

\defbeamertemplate*{frametitle}{}[1][]
{
    \begin{textblock*}{12cm}(1cm,0.75cm)
    {\color{purple} \fontsize{20}{43.2} \selectfont \insertframetitle}
    \end{textblock*}
    \begin{textblock*}{12cm}(1cm,2.5cm)
    {\color{purple} \fontsize{20}{24} \selectfont \insertframesubtitle}
    \end{textblock*}
}


\usepackage{ragged2e}

\justifying
\usepackage{lmodern}
\usepackage{ImageMagick}
\usepackage[utf8] {inputenc}
\usefonttheme[onlymath]{serif}
\usepackage[english] {label}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{bbm}
\usepackage[round] {natbib}
\usepackage{color}     
\usepackage{changepage}
\usepackage[export]{adjustbox}
\usepackage{graphicx}
\usepackage{minted}
\usepackage{listings}
\usepackage[svgnames]{xcolor}



\setbeamertemplate{footline}[frame number]

\lstdefinestyle{R} {language=R,
    stringstyle=\color{DarkGreen},
    morekeywords={TRUE,FALSE},
    deletekeywords={data,frame,length,as,character},
    keywordstyle=\color{blue},
    commentstyle=\color{teal},    
    basicstyle=\ttfamily\tiny,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=3pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=1
}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}

\lstdefinestyle{Python} {language=Python,
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\tiny,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=3pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=1}




\def\one{\mbox{1\hspace{-4.25pt}\fontsize{12}{14.4}\selectfont\textrm{1}}} % 11pt 

\makeatletter
\setbeamertemplate{frametitle}[default]{}
\makeatother
\usepackage{booktabs}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}


\begin{document}




\begin{frame}
\frametitle{Mann-Whitney-Wilcoxon test: rationale}

\vspace{30}
\noindent
Assume that we have two samples $\mathbf{X}= (x_{1},..., x_{n1})$ and $\mathbf{Y} = (y_{1},...,y_{n2})$ and we want determine if they come from the same generating distribution, that is we want to test $H_{0}:  L_{x} = L_{y}$ using a nonparametric test, i.e. without assuming one parametric distribution, e.g. Normal, Binomial... Furthermore, assume that $L_{x}$ and $L_{y}$ are both continuous distributions and that $x_{i}$ is independent of $y_{j}$ for all $i,j$. One possible solution: the \textbf{Mann-Whitney-Wilcoxon} test otherwise known as the Wilcoxon's rank sum test.

\vspace{5}
\noindent
Note: alternative versions of the Mann-Whitney-Wilcoxon test exists in case we specify the null hypothesis differently.


\vspace{5}
\noindent
Let's uncover the mathematics behind this nonparametric test and its normal approximation. In a second time, we will perform the test and some simulations in the R language.


\end{frame}




\begin{frame}
\frametitle{Basic notation and test statistic}

\vspace{40}
\noindent
Let $S=(x_{1},...,x_{n1},y_{1},...,y_{n2})$ denote the pooled sample and $R=rank(S)$, that is the ordered rank of all observation. Let us denote $N=n_{1}+n_{2}$. Finally, let us consider the r.v. $R_{X}$, which is the sum of the ranks of the observations from our first sample $x_{1},..., x_{n1}$ in the pooled sample $S$, defined as 

$$ R_{X}= \sum_{i=1}^{n1} R_{i}$$

\vspace{5}
\noindent
The Mann-Whitney-Wilcoxon test statistic is usually given by

$$
W = 2 R_{X} - n_{1} (N+1)
$$

\vspace{5}
\noindent
We are only concerned with testing $H_{0}:  L_{x} = L_{y}$.

\end{frame}




\begin{frame}
\frametitle{Distribution of the test statistic \\ and large-sample approximation}

\vspace{40}
\noindent
The distribution $W$ depends on the sample sizes $n_{1}$ and $n_{2}$. If we do not have tables which gives us a critical value to test $H_{0}$, we can estimate it using simulations. If $n_{1}$ and $n_{2}$ are large enough, there is a possibility of a 'large-sample' approximation.

\vspace{5}
\noindent
\textbf{Asymptotic approximation:} Using the CLT, we can prove that, for larges values of $n_{1}$ and $n_{2}$, $W$ has the following distribution

$$
W \sim N \bigg( 0, \ n_{1} n_{2} \frac{N+1}{3}  \bigg)
$$

\vspace{5}
\noindent
We can use the function $pnorm()$ in R to compute an approximate p-value.

\end{frame}
















\begin{frame}
\frametitle{Working example}

\vspace{50}
\noindent
\textbf{Example:} The following values are uniform samples with $\mathbf{X} \sim U_{[0, 2]}$ and $\mathbf{Y} \sim U_{[0.2, 2.2]}$ of sample sizes $n_{1} = n_{2} = 10$

$$
\begin{align*} 
X = 0.93322789, \ 0.67038191, \ 0.32563512, \ 0.79224003, \ 0.06078346, \ \\
0.24176974, \ 0.85233131, \ 1.23571576, \ 0.52641652, \ 0.95264774
\end{align*}
$$

$$
\begin{align*} 
Y = 1.9246380, \ 0.4977573, \ 0.5608602, \ 2.1985466, \ 1.8834824, \ \\ 0.4852981, \ 0.8896968, \ 1.9792509, \ 0.8396966, \ 1.6180317
\end{align*}
$$

\vspace{5}
\noindent
Using the exact distribution of W, then the asymptotic approximation, what is the conclusion of the test at a level of significance of $5 \%$ testing $H_{0}: L_{x} = L_{y}$, the data is coming from the same generating process or equivalently the temperatures are similar in both cities.

\end{frame}





\begin{frame}[ fragile]{}
\frametitle{Visualizing the data}

\vspace{50}
\noindent

 \includegraphics[scale=0.55,center]{Rplot2}


\end{frame}





\begin{frame}[ fragile]{}

\frametitle{Mann-Whitney-Wilcoxon \\ test using inbuilt function in R}

\vspace{30}
\noindent


\begin{lstlisting}[style=
R]
# 1. Mann-Whitney-Wilcoxon using inbuilt function in R

set.seed(2023)
X = runif(50, min = 0, max = 2)
Y = runif(50, min = 0.2, max = 2.2)

wilcox.test(X,Y) 
# Wilcoxon rank sum exact test
# 
# data:  X and Y
# W = 27, p-value = 0.08921
# alternative hypothesis: true location shift is not equal to 0
\end{lstlisting}

\vspace{5}
\noindent
Conclusion: we can NOT reject $H_{0}$: 'both data come from the same generating distribution' at a significance level of $5\%$ since for those samples, $p-value > 0.05$.



\end{frame}




\begin{frame}[ fragile]{}

\frametitle{Asymptotic Mann-Whitney-Wilcoxon \\ 
 test in R}

\vspace{30}
\noindent


\begin{lstlisting}[style=
R]
# 2. Asymptotic Mann-Whitney-Wilcoxon using inbuilt function in R
set.seed(2023)
X = runif(10, min = 0, max = 2)
Y = runif(10, min = 0.2, max = 2.2)
S = c(X, Y)
R = rank(S)
Rx = sum(R[1:length(X)])
W = 2*Rx + (12*(N+1))
varW = 50*50*((N+1)/3)

pnorm(W, mean = 0, sd = sqrt(varW), lower.tail = FALSE) 
# 0.0008313872

# Conclusion: using the asysmptotic approximation, we reject H0: Lx = Ly that the both
# data come from the same generating distribution.
\end{lstlisting}

\vspace{5}
\noindent
Conclusion: we can reject $H_{0}$: 'both data come from the same generating distribution' at a significance level of $5\%$ since for those samples, $p-value < 0.05$.


\end{frame}



\begin{frame}
\frametitle{Expectation of the sum of ranks and \\ sum or squared ranks}

\vspace{40}
\noindent
We know that, under  $H_{0}:  L_{x} = L_{y}$, the $R_{i}$'s are uniformly distributed on the set $\{1,...,N\}$. When computing the expectation, it involves the sum of the first $N$ integers, given by the formula $\frac{N (N+1)}{2}$. It can be proven by induction (appendix 1).

\vspace{15}
\noindent
So the expectation of the ranks and the squared ranks are respectively given by

\vspace{15}
\noindent
$E[R_{i}] = \frac{1}{N} \sum_{i=1}^{N} R_{i} = \frac{1}{N} \frac{N (N+1)}{2}=\frac{N+1}{2} $


\vspace{15}
\noindent
$E[R_{i}^2] =  \frac{1}{N} \sum_{i=1}^{N} R_{i}^{2} = \frac{1}{N} \frac{N (N+1)(2N+1)}{6} =\frac{(N+1)(2N+1)}{6} $

\par
\end{frame}








\begin{frame}[ fragile]{}

\frametitle{Variance of the ranks}

\vspace{40}
\noindent
By definition, the variance of the random variable $R_{i}$ is given by

$$var(R_{i}) = E[R_{i}^2] -\big( E[R_{i}] \big)^2$$

\vspace{5}
\noindent
Then replacing by the results that we derived on the previous slide, we get

$$var(R_{i})= \frac{(N+1)(2N+1)}{6} - \bigg( \frac{N+1}{2}   \bigg)^{2} =  \frac{N^2 -1}{12}$$


\vspace{5}
\noindent
Since $R_{i}$ is not independent from $R_{j}$, then the variance of the sum is equal to the sum of the variances plus a covariance term.

\end{frame}




\begin{frame}
\frametitle{Covariance term among ranks}

\vspace{40}
\noindent
We first note that $\sum_{i=1}^{N} R_{i}$ is a constant and therefore its variance is $0$.


 \underbrace{$var(\sum_{i=1}^{N} R_{i})}_{0} = \underbrace{ \sum_{i=1}^{N} var(R_{i}) }_{\frac{N(N^2 -1)}{12}} + \underbrace{\sum_{i=1}^{N} \sum_{j=1}^{N-1} cov(R_{i}, R_{j})}_{N (N-1)   cov(R_{i}, R_{j}) }$ \ \ \ \ \  \ \  for $i \neq j$. 


\vspace{5}
\noindent
So we have that

$$
\begin{align*} 
-\frac{N(N^2 -1)}{12} & = N(N-1)cov(R_{i}, R_{j}) \\
-\frac{N(N -1)(N+1)}{12} & = N(N-1)cov(R_{i}, R_{j}) \\
-\frac{(N+1)}{12} & = cov(R_{i}, R_{j}) 
\end{align*}
$$



\end{frame}


\begin{frame}[ fragile]{}

\frametitle{Expectation of the r.v $R_{X}$}

\vspace{15}
\noindent
To compute $E[R_{X}]$, we have:

\begin{align*} 
E[R_{X}] =& \ \sum_{i=1}^{n_{1}} E[R_{i}] \\
=& \  \  n_{1} \frac{N+1}{2} \\
=&  \  \  n_{1}   \frac{ n_{1}+n_{2} +1}{2}
\end{align*}




\end{frame}




\begin{frame}
\frametitle{Variance of the  r.v $R_{X}$ }

\vspace{40}
\noindent
To compute $var(R_{X})$, we have:


$$var(R_{X})= \sum_{i=1}^{n_{1}}var(R_{i}) + \sum_{i=1}^{n_{1}} \sum_{j=1}^{n_{1}-1}cov(R_{i},R_{j}) $$ where $ var(R_{i})= \frac{N^2 -1}{12} $ and $ cov(R_{i},R_{j})=- \frac{(N+1)}{12}$. By replacing these results in the previous equation, we get 

\begin{align*} 
var(R_{X}) =& \ \ n_{1} \ \frac{N^2 -1}{12} + n_{1} \ (n_{1} -1) \bigg(-\frac{N+1}{12} \bigg)\\
=& \ \ \frac{n_{1}}{12} \ \bigg[  N^2 -1 - (n_{1}-1) \ (N+1) \bigg] \\
=& \ \ \frac{n_{1} \ (N+1)}{12} \bigg[ N -1 - (n_{1} -1) \bigg]  \\
=& \ \ \frac{n_{1} \ n_{2}\ (N+1)}{12} 
\end{align*}




\end{frame}




\begin{frame}
\frametitle{Distribution of the  r.v $R_{X}$ }

\vspace{40}
\noindent
So we have that 

$$
R_{X} \sim N\bigg(n_{1} \frac{N+1}{2}  , \frac{n_{1} \ n_{2}\ (N+1)}{12} \bigg)
$$


\vspace{5}
\noindent
This distribution is has obviously the same shape as the distribution of $W$ since they both differ by the constant $n_{1}(N+1)$. See next slides.



\end{align*}




\end{frame}




\begin{frame}[ fragile]{}

\frametitle{Empirical distribution of $R_{X}$ and $W$ in R}

\vspace{30}
\noindent

\tiny
\par

\begin{lstlisting}[style=
R]
# distribution of Rx and W under H0 using simulation
set.seed(2023)
n = 100000
X = matrix(rep(0, 12*n), nrow = n, ncol = 12) 
Y = matrix(rep(0, 8*n), nrow = n, ncol = 8)
S = R = matrix(rep(0, (12+8)*n), nrow = n, ncol = 12+8)
Rx = W = numeric(n)

for(i in 1:n) {
  X[i,] = runif(12, min = 0, max = 2)
  Y[i,] = runif(8, min = 0, max = 2)
  S[i,] = c(X[i,], Y[i,])
  R[i,] = rank(S[i,])
  Rx[i] = sum(R[i, 1:12])
  W[i] = 2*Rx[i] + (12*(N+1))
}

# mean and variance of Rx and W
mean(Rx); var(Rx)
# [1] 126.048 about 12*(20+1)/2
# [1] 168.2862 about 12*8*(20+1)/12
mean(W); var(W)
# [1] 504.096 about (2*(12*(20+1)/2)) + 12*(20+1)
# [1] 673.145 

# plots
par(mfrow = c(1,2))
plot(density(Rx), main = 'Distribution (KDE) of Rx')
plot(density(W - rep((2*(12*(20+1)/2)) + 12*(20+1), n)), main = 'Centered distribution (KDE) of W')
\end{lstlisting}




\par
\end{frame}




\begin{frame}[ fragile]{}
\frametitle{Density plots of $R_{X}$ and $W$
} }

\vspace{50}
\noindent

 \includegraphics[scale=0.50,center]{Rplot1}


\end{frame}


\begin{frame}[ fragile]{}
\frametitle{References}

\vspace{10}
\noindent
Bagdonavičius V.,  Kruopis J., Nikulin M. S., Non-parametric Tests for Complete Data (2011), Wiley, ISBN 978-1-84821-269-5 (hardback) 

\vspace{10}
\noindent
The R Project for Statistical Computing:

\noindent
\urlf{https://www.r-project.org/}

\vspace{10}
\noindent
course notes





\par
\end{frame}










\end{document}
